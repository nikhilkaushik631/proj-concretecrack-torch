{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_dZf-N8iGd6"
      },
      "source": [
        "Importing Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VWpBsDnXiGd8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaJpclJDiGd_"
      },
      "source": [
        "Loading Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yHJOZBQLiMZO"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# URL of the dataset\n",
        "url = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/concrete_crack_images_for_classification.zip\"\n",
        "\n",
        "# Path to download the file\n",
        "zip_file_path = \"/resources/data/concrete_crack_images_for_classification.zip\"\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(os.path.dirname(zip_file_path), exist_ok=True)\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(url)\n",
        "with open(zip_file_path, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/resources/data\")\n",
        "\n",
        "# Optionally, remove the zip file after extraction\n",
        "os.remove(zip_file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CvY-26aiGeF"
      },
      "source": [
        "Dataset Class\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iQvbaYF3iGeG",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Dataset(Dataset):\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self,transform=None,train=True):\n",
        "        directory=\"/resources/data\"\n",
        "        positive=\"Positive\"\n",
        "        negative=\"Negative\"\n",
        "\n",
        "        positive_file_path=os.path.join(directory,positive)\n",
        "        negative_file_path=os.path.join(directory,negative)\n",
        "        positive_files=[os.path.join(positive_file_path,file) for file in  os.listdir(positive_file_path) if file.endswith(\".jpg\")]\n",
        "        positive_files.sort()\n",
        "        negative_files=[os.path.join(negative_file_path,file) for file in  os.listdir(negative_file_path) if file.endswith(\".jpg\")]\n",
        "        negative_files.sort()\n",
        "        number_of_samples=len(positive_files)+len(negative_files)\n",
        "        self.all_files=[None]*number_of_samples\n",
        "        self.all_files[::2]=positive_files\n",
        "        self.all_files[1::2]=negative_files\n",
        "        # The transform is goint to be used on image\n",
        "        self.transform = transform\n",
        "        #torch.LongTensor\n",
        "        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n",
        "        self.Y[::2]=1\n",
        "        self.Y[1::2]=0\n",
        "\n",
        "        if train:\n",
        "            self.all_files=self.all_files[0:10000] #Change to 30000 to use the full test dataset\n",
        "            self.Y=self.Y[0:10000] #Change to 30000 to use the full test dataset\n",
        "            self.len=len(self.all_files)\n",
        "        else:\n",
        "            self.all_files=self.all_files[30000:]\n",
        "            self.Y=self.Y[30000:]\n",
        "            self.len=len(self.all_files)\n",
        "\n",
        "    # Get the length\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    # Getter\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "\n",
        "        image=Image.open(self.all_files[idx])\n",
        "        y=self.Y[idx]\n",
        "\n",
        "\n",
        "        # If there is any transform method, apply it onto the image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrJSHu5qiGeH"
      },
      "source": [
        "Transform Object\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "obaECjMqiGeI",
        "tags": [],
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "\n",
        "transform =transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean, std)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Splitting Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kFnvt4priGeJ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "dataset_train=Dataset(transform=transform,train=True)\n",
        "dataset_val=Dataset(transform=transform,train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB9XgLKAiGeK"
      },
      "source": [
        "Shape & Size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZGYQg14AiGeK",
        "outputId": "03a88bca-714f-4130-b7a2-7199a581e216",
        "tags": []
      },
      "outputs": [],
      "source": [
        "dataset_train[0][0].shape\n",
        "size_of_image=3*227*227\n",
        "size_of_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "pStUvpwziGeO",
        "outputId": "21e8acef-641f-4579-ede5-d0767c3b8520"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2glwNT9iGeO"
      },
      "source": [
        "Custom Module\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qx5zAtLgiGeP"
      },
      "outputs": [],
      "source": [
        "class Softmax(nn.Module):\n",
        "    def __init__(self,in_size,out_size):\n",
        "        super(Softmax,self).__init__()\n",
        "        self.linear = nn.Linear(in_size,out_size)\n",
        "    def forward(self,x):\n",
        "        out = self.linear(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFTHMU8WiGeP"
      },
      "source": [
        "Model Object\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8rHUwe56iGeQ"
      },
      "outputs": [],
      "source": [
        "out_size = 2\n",
        "model = Softmax(size_of_image,out_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbZwhNnMiGeQ"
      },
      "source": [
        "Optimizer, Criterion, Learning rate, Momentum\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NZ3eXgapiGeQ"
      },
      "outputs": [],
      "source": [
        "l_r = 0.1\n",
        "p = 0.1\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=l_r, momentum = p)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DFMSnb6iGen"
      },
      "source": [
        "Data Loader \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gMg1i9OUiGen"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_loader = DataLoader(dataset=dataset_train, batch_size=100)\n",
        "validation_loader = DataLoader(dataset=dataset_val, batch_size=5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZVard6IiGeo"
      },
      "source": [
        "\n",
        "Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZFYwskCLjgia",
        "outputId": "9c89bd7f-b5fd-401e-cabb-d6ac4f716bfa"
      },
      "outputs": [],
      "source": [
        "n_epochs = 5\n",
        "accuracy_list = []\n",
        "N_test = len(dataset_val)\n",
        "max_accuracy = 0.0  # Initialize max accuracy\n",
        "\n",
        "def train_model(n_epochs, size_of_image):\n",
        "    global max_accuracy\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for x, y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "         \n",
        "            z = model(x.view(-1, size_of_image))\n",
        "            loss = criterion(z, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        correct = 0\n",
        "        # Perform a prediction on the validation data\n",
        "        for x_test, y_test in validation_loader:\n",
        "          \n",
        "            z = model(x_test.view(-1, size_of_image))\n",
        "            _, yhat = torch.max(z.data, 1)\n",
        "            correct += (yhat == y_test).sum().item()\n",
        "\n",
        "        accuracy = correct / N_test\n",
        "        accuracy_list.append(accuracy)\n",
        "\n",
        "        # Update max accuracy if current epoch's accuracy is higher\n",
        "        if accuracy > max_accuracy:\n",
        "            max_accuracy = accuracy\n",
        "\n",
        "train_model(n_epochs, size_of_image)\n",
        "print(f\"Maximum Accuracy Achieved: {max_accuracy}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "conda-env-python-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "prev_pub_hash": "d76b47f27f3838cee5fcf531fe9ce8abc439f204aaa935161f67b638f12e7c04"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
